# app.py
import streamlit as st
import pandas as pd
import numpy as np
import json
import altair as alt
from sentence_transformers import util

from utils import (
    preprocess_text, read_uploaded_file_bytes, parse_topics_field,
    jaccard_tokens, style_suspicious_and_low, simple_flags, pos_first_token,
    bootstrap_diff_ci
)
from model_utils import load_model_from_source, encode_texts_in_batches

# ==============================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
# ==============================
st.set_page_config(page_title="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫", layout="wide")
st.title("üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ñ—Ä–∞–∑–∞–º")

# ==============================
# –°–∞–π–¥–±–∞—Ä ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
# ==============================
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")
source = st.sidebar.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏", ["huggingface", "google_drive"])
identifier = st.sidebar.text_input("ID –º–æ–¥–µ–ª–∏ / –ø—É—Ç—å", value="paraphrase-multilingual-MiniLM-L12-v2")

if st.sidebar.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å"):
    with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏..."):
        model = load_model_from_source(source, identifier)
        st.session_state["model"] = model
        st.success("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# ==============================
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
# ==============================
uploaded_files = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã (CSV, Excel, JSON)", type=["csv", "xlsx", "xls", "json", "ndjson"],
    accept_multiple_files=True
)

dataframes = []
hashes = []
if uploaded_files:
    for f in uploaded_files:
        try:
            df, h = read_uploaded_file_bytes(f)
            dataframes.append(df)
            hashes.append(h)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {f.name}: {e}")

# ==============================
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
# ==============================
st.sidebar.header("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")
query = st.sidebar.text_input("–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
semantic_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏–∫–∏", 0.0, 1.0, 0.7, 0.01)
lexical_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –ª–µ–∫—Å–∏–∫–∏", 0.0, 1.0, 0.3, 0.01)
low_score_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –Ω–∏–∑–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞", 0.0, 1.0, 0.5, 0.01)
top_k = st.sidebar.number_input("–°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å", 1, 100, 10)

# ==============================
# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
# ==============================
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫"):
    if "model" not in st.session_state:
        st.error("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å")
    elif not dataframes:
        st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª")
    elif not query.strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
    else:
        model = st.session_state["model"]

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã
        combined_df = pd.concat(dataframes, ignore_index=True)
        if "phrase" not in combined_df.columns:
            st.error("–í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'phrase'")
        else:
            combined_df["phrase_proc"] = combined_df["phrase"].map(preprocess_text)

            # –ö–æ–¥–∏—Ä—É–µ–º
            with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤..."):
                query_emb = encode_texts_in_batches(model, [query])[0]
                corpus_embs = encode_texts_in_batches(model, combined_df["phrase_proc"].tolist())

            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            scores = util.cos_sim(query_emb, corpus_embs)[0].cpu().numpy()

            # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            lexical_scores = [jaccard_tokens(query.lower(), p) for p in combined_df["phrase_proc"]]

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É
            results_df = combined_df.copy()
            results_df["score"] = scores
            results_df["lexical_score"] = lexical_scores

            # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –Ω–∏–∑–∫–∏—Ö
            styled = style_suspicious_and_low(results_df, semantic_threshold, lexical_threshold, low_score_threshold)
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            st.dataframe(styled, use_container_width=True)

            # –ú–µ—Ç—Ä–∏–∫–∏
            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏")
            st.write(f"–°—Ä–µ–¥–Ω–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {np.mean(scores):.3f}")
            st.write(f"–°—Ä–µ–¥–Ω–µ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {np.mean(lexical_scores):.3f}")

            # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            chart_data = pd.DataFrame({
                "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ": scores,
                "–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ": lexical_scores
            })
            chart = alt.Chart(chart_data.reset_index()).mark_circle(size=60).encode(
                x="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ", y="–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ"
            )
            st.altair_chart(chart, use_container_width=True)
